{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio di web scraping Saul Urso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "qYrYrFO7BzmuYCmUjzsWCC",
     "type": "MD"
    }
   },
   "source": [
    "### Note su funzionamento notebook\n",
    "* Il notebook assume che sia presente nella propria directory corrente la directory \"2013\" contente il dataset. **Attenzione**: la directory non è nello zip fornito, data l'impossibilità di poter inviare un file di dimensioni così grandi su moodle.\n",
    "* Riguardo l'esercizio **2.1**:\n",
    "    * Ogni punto dell'esercizio ( a, b, ecc...) assume siano stati effettuati gli import e siano stati creati i dataframe dalle due celle sotto il titolo \"Esercizio 2.1\";\n",
    "    * Si possono eseguire i vari punti in qualunque ordine (le celle all'interno di un punto vanno eseguite nell'ordine in cui sono presenti);\n",
    "    * Per poter mostrare i risultati senza dover eseguire il notebook, alla fine di ogni punto è stata messa un'immagine del grafico prodotto.\n",
    "* Riguardo l'esercizio **2.2**:\n",
    "    * Per una corretta esecuzione eseguire le celle nell'ordine in cui sono presenti (**non serve** eseguire le celle antecedenti al titolo dell'esercizio);\n",
    "    * **Attenzione**: l'esecuzione dell'algoritmo di clustering potrebbe richiedere del tempo (e/o richiede molta RAM), sul mio pc è durata 3 minuti ma su altri potrebbe essere molto peggio.\n",
    "\n",
    "* Riguardo l'esercizio **2.3**:\n",
    "    * L'esercizio assume che siano state ottenute le componenti connesse tramite il punto **2.2** e che sia stato generato il file \"Components.json\";\n",
    "    * Per non dover eseguire il punto **2.2**, recuperare il file json a https://drive.google.com/file/d/1ZdLgBX-0f1so-CHWL3ubCVRe5AT1zaoI/view?usp=sharing e metterlo nella directory del noteboook.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attenzione\n",
    "\n",
    "* eseguire la cella sottostante prima di procedere all'esecuzione delle restanti celle del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questa cella rinomina le varie colonne del dataset, per poter essere utilizzate successivamente.\n",
    "#Le tabelle con le colonne rinominate saranno salvate nella directory \"dataset\"\n",
    "#che verrà creata se non è già presente.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "print('start')\n",
    "df_inputs = pd.read_csv(\"2013/inputs.csv\")\n",
    "df_transactions = pd.read_csv(\"2013/transactions.csv\")\n",
    "df_outputs = pd.read_csv(\"2013/outputs.csv\")\n",
    "df_mapping = pd.read_csv(\"2013/mapAddr2Ids8708820.csv\")\n",
    "\n",
    "try:\n",
    "    os.mkdir('dataset')\n",
    "except FileExistsError:\n",
    "    print('Directory già esiste')\n",
    "    \n",
    "df_inputs.columns= ['txId','prevTxId','prevTxpos']\n",
    "df_outputs.columns= ['txId','position','addressId','amount','scripttype']\n",
    "df_transactions.columns= ['timestamp','blockId','txId','isCoinbase','fee']\n",
    "df_mapping.columns= ['hash','addressId']\n",
    "\n",
    "df_inputs.to_csv(\"dataset/inputs.csv\",index=False)\n",
    "df_transactions.to_csv(\"dataset/transactions.csv\",index=False)\n",
    "df_outputs.to_csv(\"dataset/outputs.csv\",index=False)\n",
    "df_mapping.to_csv(\"dataset/mapAddr2Ids8708820.csv\",index=False)\n",
    "\n",
    "print('end')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Esercizio 2.1",
     "sheet_delimiter": true,
     "type": "MD"
    }
   },
   "source": [
    "# Esercizio 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "p8sKZyVYRTsm3iLzvwgEh1",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "#eseguire per 2.1\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eseguire per 2.1\n",
    "\n",
    "df_inputs = pd.read_csv(\"dataset/inputs.csv\")\n",
    "df_transactions = pd.read_csv(\"dataset/transactions.csv\")\n",
    "df_outputs = pd.read_csv(\"dataset/outputs.csv\")\n",
    "df_mapping = pd.read_csv(\"dataset/mapAddr2Ids8708820.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "sGOgxbbxdS27U83bwLhMvu",
     "type": "CODE"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_mapping.info()\n",
    "df_mapping.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "zqZcAokTjiTRiYZJ8PCsLr",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "df_inputs.info()\n",
    "df_inputs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outputs.info()\n",
    "df_outputs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions.info()\n",
    "df_transactions.head(5)\n",
    "df_transactions.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "HWRsA3VOlGikZYyVrLbpZW",
     "type": "MD"
    }
   },
   "source": [
    "## a) distribuzione del numero di transazioni per blocco (occupazione del blocco), nell’intero periodo temporale considerato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Vn4XoBoPT4bYWrHsvnMatr",
     "type": "MD"
    }
   },
   "source": [
    "### Cosa fare\n",
    "1. Prendere la tabella delle transazioni;\n",
    "2. Raggruppare per numero blocco **blockId**;\n",
    "3. Contare quante transazioni ci sono per blocco;\n",
    "4. Plottare su istogramma dati (visto che plotto su istogramma basta avere il numero di transazioni per blocco, alle frequenze ci pensa lui)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=20 #ampiezza bin, rappresenta che dimensioni sono presenti in quel bin\n",
    "\n",
    "\n",
    "block_counts= df_transactions.groupby(\"blockId\").size().reset_index(name='counts') #2,3\n",
    "\n",
    "#4\n",
    "\n",
    "x= block_counts['counts']\n",
    "\n",
    "plt.gcf().set_size_inches(25,10) #imposto dimesioni figura\n",
    "\n",
    "#imposto vari titoli\n",
    "\n",
    "plt.title(\"Distribuzione del numero di transazioni per blocco\",fontsize=24)\n",
    "plt.xlabel(\"Numero di transazioni\",fontsize=18,)\n",
    "plt.ylabel(\"Numero di blocchi\",fontsize=18)\n",
    "\n",
    "plt.hist(x,bins=round(max(x)/n),log=True,edgecolor='black') #plotto istogramma\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Le frequenze sono molto più sbilanciate in realtà, visto che sto utilizzando la scala logaritmica\n",
    "#dalla documentazione-> log: If True, the histogram axis will be set to a log scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/2.1a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Evoluzione dell'occupazione dei blocchi nel tempo, considerando intervalli temporali di due mesi. In questo caso produrre un grafico che riporti il numero di transazioni medie per ogni periodo considerato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosa fare\n",
    "\n",
    "1. Prendere tabella transazioni\n",
    "2. Raggruppare per numero blocco **blockId**;\n",
    "3. Ottenere numero transazioni per blocco\n",
    "4. Tenere conto di timestamp blocco ( tutte le transazioni quando raggruppo avranno stesso timestamp -> basta prenderne uno);\n",
    "5. Creare lista contente medie sui vari intervalli (valori da plottare su asse y)\n",
    "6. Plottare medie ottenute (lineplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "block_counts= df_transactions.groupby(\"blockId\").size().reset_index(name='counts') #2,3\n",
    "block_stamps = df_transactions.groupby(\"blockId\")['timestamp'].first().reset_index(name='timestamp') #4\n",
    "\n",
    "data = block_counts.merge(block_stamps,on= 'blockId') #probabilmente join non necessaria.\n",
    "#la faccio per essere sicuro che timestamp e count combacino, fisto che ho fatto 2 query separate\n",
    "# e l'ordinamento potrebbe essere diverso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#5\n",
    "\n",
    "#prendo il primo e l'ultimo timestamp\n",
    "min_stamp=data['timestamp'].min()\n",
    "max_stamp = data['timestamp'].max()\n",
    "\n",
    "#li converto in date\n",
    "start = datetime.datetime.utcfromtimestamp(min_stamp)\n",
    "start= datetime.datetime(start.year,start.month,day=1) #metto la data di partenza a inizio del mese\n",
    "\n",
    "end = datetime.datetime.utcfromtimestamp(max_stamp)\n",
    "end= datetime.datetime(end.year+1,start.month,1) #allineo con inizio mese successivo, serve per pd.date_range\n",
    "\n",
    "\n",
    "#creo lista contenente un mese ogni 2\n",
    "\n",
    "#prima ottengo lista contenente tutti i mesi\n",
    "dates_list= pd.date_range(start.date(),end.date(),freq='MS').to_pydatetime() \n",
    "\n",
    "#nel mentre preparo già parte dell'asse delle ascisse per il plot\n",
    "x_ax= [el.date() for idx,el in enumerate(dates_list) if idx % 2 == 1]\n",
    "\n",
    "#poi tolgo un mese alterno (intervalli di 2 mesi)\n",
    "dates_list= [ el.date() for idx,el in enumerate(dates_list) if idx % 2 == 0] \n",
    "\n",
    "#converto colonna timestamp in date\n",
    "data['dates']= pd.to_datetime(data['timestamp'],unit='s').dt.date\n",
    "\n",
    "\n",
    "#creazione lista medie\n",
    "\n",
    "mean_list = list()\n",
    "\n",
    "for idx,el in enumerate(dates_list): #calcolo la media su 2 mesi dell'occupazione dei blocchi\n",
    "   \n",
    "    if el == end.date() : #ho finito\n",
    "        break\n",
    "    \n",
    "    #prendo altro estremo dell'intervallo  \n",
    "    next= dates_list[idx+1] \n",
    "    \n",
    "    #creo maschera per selezione (date devono essere frai 2 estremi)\n",
    "    mask= (data['dates']>= el) & (data['dates']<= next)\n",
    "    my_query= data[mask] #seleziono tutti i blocchi compresi frai 2 mesi\n",
    "    \n",
    "    total_tra = my_query.counts.sum() #numero tot transazioni in intervallo\n",
    "    \n",
    "    block_num = my_query['blockId'].size #numero blocchi\n",
    "    \n",
    "    #aggiungo a lista media transazioni per blocco in quei 2 mesi\n",
    "    mean_list.append(float(total_tra)/float(block_num)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#6\n",
    "\n",
    "#devo plottare medie su 2 mesi\n",
    "#la media la riporto con ascissa a metà dell'intervallo dei 2 mesi (la data che divide l'intervallo a metà)\n",
    "#per gli estremi (primo giorno e ultimo giorno) riporto la media di quel giorno\n",
    "\n",
    "#inserisco i 2 estremi tra le ascisse\n",
    "min_stamp=data['timestamp'].min()\n",
    "start = datetime.datetime.utcfromtimestamp(min_stamp).date()\n",
    "x_ax.insert(0,start)\n",
    "\n",
    "max_stamp = data['timestamp'].max()\n",
    "end = datetime.datetime.utcfromtimestamp(max_stamp).date()\n",
    "x_ax.append(end)\n",
    "\n",
    "#inserisco le 2 medie giornaliere (primo e ultimo giorno) fra le ordinate\n",
    "\n",
    "my_query= data[data['dates']==start]\n",
    "total_tra = my_query.counts.sum()\n",
    "block_num = my_query['blockId'].size\n",
    "\n",
    "mean_list.insert(0,float(total_tra)/float(block_num))\n",
    "\n",
    "my_query= data[data['dates']==end]\n",
    "total_tra = my_query.counts.sum()\n",
    "block_num = my_query['blockId'].size\n",
    "\n",
    "mean_list.append(float(total_tra)/float(block_num))\n",
    "\n",
    "#effettuo plotting\n",
    "plt.gcf().set_size_inches(25,10) #imposto dimesioni figura\n",
    "\n",
    "plt.xticks(rotation=270) #mette date ruotate\n",
    "plt.title('Evoluzione dell\\'occupazione dei blocchi nel tempo',fontsize=24)\n",
    "plt.xlabel('Mesi',fontsize=18)\n",
    "plt.ylabel(\"Transazioni per blocco\",fontsize=18)\n",
    "plt.plot(x_ax,mean_list,marker='o')\n",
    "plt.xticks(x_ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/2.1b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Ammontare totale degli UTXO al momento dell’ultima transazione registrata nella blockchain considerata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per essere UTXO bisogna che l'output di una transazione non sia input di un'altra.\n",
    "Notiamo che ogni output mantiene in particolare 2 cose:\n",
    "* il riferimento alla transazione che lo ha generato\n",
    "* la sua posizione fra gli output di tale transazione\n",
    "\n",
    "che lo identificano univocamente (nessun altro output può avere tale coppia uguale).\n",
    "Lo stesso discorso vale per gli input.\n",
    "Ma, un input non è altro che un output speso (non UTXO).\n",
    "Quindi, per vedere se un output è UTXO, basta verificare che non ci sia un input con la stessa coppia (transazione di provenienza, posizione).\n",
    "\n",
    "Ma allora basta effettuare una join right fra le due tabelle inputs e outputs (outputs a dx) su questa coppia, e le righe che avranno i campi di inputs a NaN saranno UTXO (l'output non compare mai come input).\n",
    "\n",
    "### Cosa fare\n",
    "\n",
    "1. Prendere le 2 tabelle inputs e outputs\n",
    "2. Effettuare join right\n",
    "3. Selezionare righe con attributi di input nulli\n",
    "4. Effettuare somma su attributo **amount**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1,2\n",
    "all_results= df_inputs.merge(df_outputs, how='right', left_on=['prevTxId','prevTxpos'], right_on= ['txId','position'])\n",
    "\n",
    "#3, che attributo di df_inputs scelgo da controllare è indifferente, sono NaN nella stessa condizione\n",
    "UTXO_table = all_results[all_results['prevTxId'].isnull()]\n",
    "\n",
    "UTXO_table['amount'].sum() #4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Distribuzione degli intervalli di tempo che intercorrono tra la transazione che genera un valore in output (UTXO) e quella che lo consuma, per gli output spesi nel periodo considerato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Cosa fare\n",
    "\n",
    "Gli output spesi sono tutti quelli presenti nella tabella inputs, quindi:\n",
    "\n",
    "1. Effettuare join inputs-transazioni su **prevTxId** (ottengo timestamp del blocco da cui proviene input);\n",
    "2. Effettuare join inputs-transazioni su **txId** (ottengo timestamp blocco in cui vengo speso);\n",
    "3. Fare la differenza, per ogni riga, dei 2 timestamp (ottengo intervallo di tempo);\n",
    "4. Plottare su istogramma distribuzione della durata degli intervalli di tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "\n",
    "join1 = df_inputs.merge(df_transactions,how='inner', left_on= ['prevTxId'], right_on=['txId']) \n",
    "#join1 = join1[['txId_x','prevTxId','prevTxpos','timestamp']]\n",
    "join1 = join1.rename({'timestamp' : 'prevTime', 'txId_x':'txId'},axis='columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "\n",
    "join1 = join1.merge(df_transactions,how='inner', on= 'txId')\n",
    "#join1 = join1[['txId','prevTxId','prevTxpos','prevTime','timestamp']]\n",
    "join1.rename({'timestamp' : 'endTime'},axis = 'columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 calcolo intervalli di tempo\n",
    "\n",
    "time_unit= 60*60*24 #1 giorno\n",
    "\n",
    "\n",
    "#considero solo gli intervalli di tempo positivi\n",
    "mask = (join1['endTime'] > join1['prevTime'])\n",
    "join1['intervals'] = 0 #inizializzo a 0\n",
    "join_valid = join1[mask]\n",
    "join1.loc[mask,'intervals'] = join_valid['endTime'] - join_valid['prevTime'] #calcolo intervalli positivi\n",
    "join1.loc[mask,'intervals'] = join1['intervals'] // time_unit #cambio unità di misurA da s a time_unit\n",
    "\n",
    "join_valid = join1[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 creo istogramma frequenze\n",
    "\n",
    "plt.gcf().set_size_inches(25,10) #imposto dimesioni figura\n",
    "\n",
    "bin_size=20 #numero di giorni (relativi alla durata dell' intervallo) per bin\n",
    "\n",
    "plt.title(\"Distribuzione degli intervalli di tempo che intercorrono tra la transazione che genera un valore in output (UTXO) e quella che lo consuma\",fontsize=18)\n",
    "plt.xlabel(\"Durata di tempo intervallo (giorni)\",fontsize=14)\n",
    "plt.ylabel(\"Numero di intervalli\",fontsize=14)\n",
    "\n",
    "plt.hist(join_valid['intervals'],bins=join_valid['intervals'].max()//bin_size,log=True,edgecolor='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/2.1d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Analisi a piacere: Numero di transazioni e quantità di fee medie, per ognuno dei 12 mesi, sull' intero periodo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosa fare\n",
    "\n",
    "1. Ottenere mese di ogni transazione\n",
    "2. Raggruppare per ogni mese e calcolare numero transazioni e somma fee\n",
    "3. Plottare su bar plot\n",
    "\n",
    "#### Perché può essere interessante\n",
    "\n",
    "* Permette di vedere se un maggior numero di transazioni implica sempre un maggior numero di fee. \n",
    "* Permette di vedere se in determinati mesi (o una determinata stagione) si registra un maggior numero di transazioni/ maggior quantità di fee. Se, per esempio, ci fosse un picco nei mesi di Giugno/Luglio/Agosto potremmo osservare che il mercato dei Bitcoin è più attivo d'estate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "df_transactions['month'] = pd.to_datetime(df_transactions['timestamp'],unit='s').dt.month \n",
    "\n",
    "#2\n",
    "\n",
    "months_table= df_transactions.groupby('month').agg(\n",
    "    totFees= ('fee', np.sum),\n",
    "    totTx= ('txId','size')\n",
    ").reset_index() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "\n",
    "#imposto assi\n",
    "\n",
    "x=months_table['month']\n",
    "months_list= [calendar.month_name[el] for el in x] #etichette per asse x\n",
    "y1= (months_table['totFees']/4).apply(np.log)\n",
    "y2= (months_table['totTx']/4).apply(np.log) #2009-12 sono 4 anni quindi /4\n",
    "width=0.2 #larghezza barre\n",
    "\n",
    "plt.gcf().set_size_inches(25,14) #imposto dimesioni figura\n",
    "\n",
    "plt.bar(x-0.1,y1,width,color='blue',label='fees')\n",
    "plt.bar(x+0.1,y2,width,color='orange',label='Tx')\n",
    "plt.xticks(x,months_list,fontsize=18)\n",
    "plt.yticks(np.arange(0,26,1))\n",
    "\n",
    "plt.title(\"Numero di transazioni e quantità di fee per ogni mese\",fontsize=24)\n",
    "plt.xlabel('months',fontsize=18)\n",
    "plt.ylabel('quantity (ln)',fontsize=18)\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/2.1e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs = pd.read_csv(\"dataset/inputs.csv\")\n",
    "df_outputs = pd.read_csv(\"dataset/outputs.csv\")\n",
    "df_mapping = pd.read_csv(\"dataset/mapAddr2Ids8708820.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementare un algoritmo di clustering che realizzi l'euristica descritta. \n",
    "\n",
    "* Viste le dimensioni del DataSet è necessario limitare la complessità dell’algoritmo, in particolare si dovrà implementare un algoritmo di complessità lineare rispetto al numero di indirizzi diversi contenuti nel DataSet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spiegazione dell'algoritmo\n",
    "\n",
    "Dal punto di vista teorico, l'algoritmo di clustering per l' euristica richiesta si basa sul creare un grafo a partire dai dati, e si compone delle seguenti fasi:\n",
    "\n",
    "* Aggiungere tutti gli indirizzi come nodi nel grafo;\n",
    "* Per ogni transazione, aggiungere un arco dal primo input verso tutti gli altri input della transazione;\n",
    "* Calcola le componenti debolmente connesse.\n",
    "\n",
    "Le componenti debolmente connesse ottenute da quest'algoritmo saranno proprio i cluster secondo l'euristica richiesta. Per implementare quest' algoritmo però bisogna effettuare alcuni passaggi in più di manipolazione dati su tabelle. I passaggi che avvengono nell'implementazione sono i seguenti:\n",
    "\n",
    "1. Effettuare join fra tabelle outputs e mapping, per recuperare hash di ogni output;\n",
    "2. Effettuare join fra tabella ottenuta da *1* e tabella inputs( è l'oggetto df_inputs, riuso le stesse variabili per risparmiare in memoria) ottenendo le coppie (txId,hash), che mi indicano che quell' hash è input di quella transazione;\n",
    "3. Per ogni riga in df_inputs:\n",
    "    1. se è la prima volta che vedo la transazione, inserisco in una mappa la coppia (txId,hash) di quella riga;\n",
    "    2. se la transazione è presente nella mappa (vista almeno una volta), recupero l'hash relativo e aggiungo l'arco (primo hash, hash corrente) al grafo;\n",
    "4. Aggiungo al grafo i rimanenti nodi isolati scorrendo la tabella mapping;\n",
    "5. Cerco le componenti connesse (il grafo che ho creato è indiretto; tuttavia cercare componenti connesse su grafi indiretti è la stessa cosa che cercare componenti debolmente connesse su grafi diretti);\n",
    "\n",
    "#### Analisi\n",
    "\n",
    "Ecludendo le join (merge) iniziali l'algoritmo scorre tutta la tabella di input nello step *3*. Per ogni entry della tabella, controlla se il txId (chiave) era già prensente del dictionary. Questa operazione, è $O(1)$, visto che i dizionari sono implementati con una hashtable. Pertanto il passo $3$ ha complessità $O(Inputs)$. (Assumendo il caso medio per la complessità di tempo delle hashtable)\n",
    "\n",
    "Il passo 4 ha complessità $O(Address)$, visto che scorro tutta la tabella degli indirizzi.\n",
    "\n",
    "Il passo 5, non sapendo come è implementato su NetworkX, ha, alla peggio, complessità $O(Address + Inputs)$. (Informandomi però probabilmente ha solo complessità $O(Address)$)\n",
    "\n",
    "Pertanto l'algoritmo ha complessità $O(Inputs + Address)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.process_time()\n",
    "print(\"started\")\n",
    "\n",
    "\n",
    "#1\n",
    "df_outputs= df_outputs.merge(df_mapping,on='addressId') #join passo 1\n",
    "df_outputs= df_outputs[['txId','position','hash']] #proietto solo colonne interessanti\n",
    "\n",
    "#2\n",
    "df_inputs= df_inputs.merge(df_outputs, left_on=['prevTxId','prevTxpos'], right_on= ['txId','position']) #join passo 2\n",
    "df_inputs= df_inputs[['txId_x','hash']] #proietto solo colonne interessanti\n",
    "df_inputs= df_inputs.rename({'txId_x' : 'txId'},axis='columns') #rinomino per chiarezza\n",
    "\n",
    "#txId mi dice di che transazione sono input\n",
    "#hash è l'hash associato\n",
    "\n",
    "#Dictionary che contiene coppie (txId,primo hash relativo)\n",
    "tx_map = defaultdict(lambda : \"\")\n",
    "\n",
    "G= nx.Graph()\n",
    "\n",
    "print(\"Start loop\")\n",
    "\n",
    "\n",
    "#3\n",
    "for row in df_inputs.itertuples(name=None):\n",
    "    \n",
    "    curr= tx_map[row[1]] #accedo a tx_map['txId'], mi restituisce hash relativo\n",
    "    \n",
    "    if curr == \"\" : #priva volta vedo questa transazione, non devo aggiungere archi\n",
    "        tx_map[row[1]]= row[2] #row[2] contiene hash\n",
    "        continue\n",
    "        \n",
    "    #avevo già visto un hash relativo a questa transazione    \n",
    "    #curr sarà quell'hash\n",
    "    #devo avere arco in G frai 2 hash\n",
    "    \n",
    "    G.add_edge(curr,row[2])\n",
    "    \n",
    "#a questo punto ho aggiunto tutte le componenti connesse, devo aggiungere i nodi isolati\n",
    "#step 4\n",
    "G.add_nodes_from(df_inputs['hash'])\n",
    "\n",
    "#Grafo completato, ora devo trovare tutte le componenti debolmente connesse \n",
    "\n",
    "print(\"Start components\")\n",
    "\n",
    "#step 5\n",
    "\n",
    "my_components= [c for c in sorted(nx.connected_components(G),key=len,reverse=True)]\n",
    "my_components_len = [len(c) for c in my_components]\n",
    "\n",
    "end=time.process_time()\n",
    "print(end-start) #187.2203370000002\n",
    "\n",
    "\n",
    "#per fare dump grafo scommentare le due righe sotto; ci mette tanto\n",
    "\n",
    "#with open(\"Graph.json\",'w') as outfile :\n",
    " #   json.dump(nx.adjacency_data(G),outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per fare dump cluster\n",
    "\n",
    "with open('Components.json','w') as outfile:\n",
    "    json.dump([list(c) for c in my_components],outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produrre alcune statistiche descrittive del clustering ottenuto (dimensione media, minima e massima dei cluster, distribuzione delle loro dimensioni, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numero di cluster\n",
    "print(f\"number of clusters: {len(my_components_len)}\")\n",
    "\n",
    "#dimensione massima\n",
    "print(f\"max cluster size: {my_components_len[0]}\")\n",
    "\n",
    "#dimensione minima\n",
    "print(f\"min cluster size: {my_components_len[-1]}\")\n",
    "\n",
    "mean= sum(my_components_len)/len(my_components)\n",
    "\n",
    "print(f\"average cluster size: {mean}\")\n",
    "\n",
    "#percentuale cluster di dimensione 1\n",
    "ones= len([1 for c_len in my_components_len if c_len == 1])\n",
    "print(f\"Percentage of clusters with 1 node only: {ones/len(my_components_len)*100 : .2f}\")\n",
    "\n",
    "#distribuzione dimensioni\n",
    "\n",
    "#calcolo frequenze\n",
    "\n",
    "appearances = defaultdict(int)\n",
    "\n",
    "for curr in my_components_len:\n",
    "    appearances[curr] += 1\n",
    "\n",
    "#plot\n",
    "    \n",
    "plt.gcf().set_size_inches(25,14) #imposto dimesioni figura\n",
    "plt.xscale('log')\n",
    "plt.ylabel('numero di cluster',fontsize=18)\n",
    "plt.xlabel('Dimensione dei cluster',fontsize=18)\n",
    "plt.title('Distribuzione della dimensione dei cluster',fontsize=24)\n",
    "\n",
    "plt.xscale('log') #scala log\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.plot([key for key in appearances],list(appearances.values()),marker='x',color='black',lw=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/2.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WalletExplorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosa fare\n",
    "\n",
    "Le pagine web di WalletExplorer non fanno uso di javascript, pertanto beautifulsoup risulta sufficiente.\n",
    "\n",
    "Per capire cosa fare, iniziamo guardando la pagina iniziale di WalletExplorer. Notiamo che sono presenti due barre di ricerca, una nel centro della pagina e una nella toolbar in alto a dx.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/WE1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provando a interagire per ricercare indirizzi possono accadere due cose:\n",
    "\n",
    "* l' indirizzo è stato trovato con successo, e verrà mostrata la pagina del wallet relativo\n",
    "* l'indirizzo non è presente, in quel caso verrà mostrata la seguente pagina:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/WE2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo nella barra di ricerca browser la query che è stata effettuata al server: non è altro che l'assegnamento al parametro 'q' dell'address ricercato. Pertanto nel nostro script potremo riassegnare ogni voltaquel parametro, senza dover interagire con i form della pagina principale.\n",
    "\n",
    "Quando invece una ricerca di un address va a buon fine, verremo portati alla pagina del wallet di cui fa parte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/WE3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizzando il codice HTML relativo, notiamo che il nome del wallet si trova all'interno di un header h2 (in particolare è il terzo figlio). Pertanto a noi basta recuperare l' header h2, e il terzo figlio sarà proprio il nome del wallet che stavamo cercando per la procedura di deanonimizzazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/WE4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putroppo non sempre, dopo aver eseguito questa procedura, WalletExplorer restituirà un nome significativo. In alcuni casi è possibile restituisca una sequenza del tipo '[numero_esadecimale]'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/WE5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data l'impossibilità di fare un' analisi su tutti gli indirizzi di ogni cluster ( dopo poco tempo che si prova a fare richieste WalletExplorer blocca l'indirizzo ip) lo script, come riprova, recupera il numero di address associato al cluster su WalletExplorer. \n",
    "\n",
    "Per fare ciò notiamo che, nella pagina di un wallet è presente, accanto al nome del wallet un link che rimanda alla pagina dove sono presenti tutti gli address relativi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/WE6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attenzione**: non tutte le pagine presentano solo il link agli address, alcune presentano anche un link al servizio/organizzazione relativo). In questo caso il link agli address del wallet sarà l'ultimo dei link presenti nell' header h2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/WE9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se proviamo ad andare su questo link, verremo rimandati alla pagina degli address di quel wallet,\n",
    "dove è presente la scritta '(total addresses: numero_address)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/WE7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizzando il codice HTML della pagina, notiamo che il numero di address si trova dentro l'unico tag \\<small> all'interno del primo div con $class = paging$ all'interno del div con $id = main$, potendolo così recuperare facilmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/WE8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importo cluster che mi ero creato nel punto 2.2\n",
    "\n",
    "with open('Components.json','r') as infile:\n",
    "    my_components= json.load(infile)\n",
    "my_components_len= [len(c) for c in my_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prendo i 10 cluster più grandi\n",
    "n= 10 #numero cluster\n",
    "\n",
    "chosen_clusters = my_components[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione che mi restituisce il nome del wallet su 'https://www.walletexplorer.com/' relativo al cluster \n",
    "def solve_name(cluster):\n",
    "    \n",
    "    base_url = 'https://www.walletexplorer.com/' #url base del sito\n",
    "    \n",
    "    #headers per fingersi umano,\n",
    "    #non necessari ma li ho usati per cercare di aggirare il rate massimo di richieste (senza successo)\n",
    "    headers= {'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:101.0) Gecko/20100101 Firefox/101.0'}\n",
    "    \n",
    "    for address in cluster: #per ogni indirizzo del cluster\n",
    "        \n",
    "        try : \n",
    "            \n",
    "            #effettuo query per trovare wallet address\n",
    "            r= requests.get(base_url,params={'q' : address},headers=headers) \n",
    "            \n",
    "            #controllo se richiesta avvenuta con successo, solleva eccezione trattata in except\n",
    "            r.raise_for_status() \n",
    "            \n",
    "            \n",
    "            bs= BeautifulSoup(r.text,'html.parser')\n",
    "            error= bs.find('p',class_='error') #controllo se ha trovato wallet address\n",
    "            #la find mi dice se è presente il paragrafo rosso, che indica fallimento, \n",
    "            \n",
    "            if error is not None: #impossibile trovare wallet\n",
    "                continue #vado a quello dopo\n",
    "            \n",
    "            #wallet trovato\n",
    "            #recupero header h2\n",
    "            title= bs.find('h2')\n",
    "            \n",
    "            \n",
    "            #parte codice commentata per cercare di deanonimizzare, saltando nomi non significativi (senza successo)\n",
    "            #if list(title.children)[2][0] == '[':\n",
    "            #    continue\n",
    "            \n",
    "            #nome del wallet è il terzo figlio\n",
    "            wallet_name= list(title.children)[2]\n",
    "            \n",
    "            #cerco tutti i link nell'header h2\n",
    "            a=title.find_all('a',href=True) \n",
    "            \n",
    "            \n",
    "            #l'ultimo link è quello della pagina degli address, procedo a richiederla\n",
    "            r=requests.get(base_url+a[-1]['href'],headers=headers)\n",
    "            bs= BeautifulSoup(r.text,'html.parser')\n",
    "            \n",
    "            add_num= bs.find('div',id='main').find('div',class_='paging').find('small').text #numero address wallet\n",
    "            \n",
    "            return (wallet_name,add_num) #restituisco nome e dimensioni\n",
    "            \n",
    "        except requests.exceptions.RequestException as e: #errore di qualche tipo\n",
    "            print(e)\n",
    "            continue\n",
    "    \n",
    "    return None        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deanonimizzo i cluster scelti\n",
    "\n",
    "names_list_we= list()\n",
    "\n",
    "for idx,cluster in enumerate(chosen_clusters):\n",
    "    print(idx)\n",
    "    \n",
    "    names_list_we.append(solve_name(cluster))\n",
    "    \n",
    "    print(names_list_we[idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#mostro nomi e dimensioni dei cluster\n",
    "\n",
    "for idx,(my_len,(name,new_len))in enumerate(zip(my_components_len,names_list_we)):\n",
    "    print(f'Cluster {idx} size: {my_len} \\tName: {name}\\t\\t{new_len}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/res1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bitcoininfocharts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebbene Bitinfocharts presenti diverse parti in javascript, il nome del wallet relativo ad un indirizzo è già presente nel codice html della pagina, pertanto beautifulsoup risulta sufficiente. Tuttavia Bitinfocharts fa uso di captcha, che vengono creati dinamicamente. \n",
    "\n",
    "Sono proposti due script. Il **primo**,realizzato con beautifulsoup, mostra che la deanonimizzazione è possibile senza selenium ( lo script rimarrà bloccato indefinitamente in caso di captcha). Il **secondo**, realizzato con selenium, quando incontra un captcha passa all'indirizzo successivo, nel tentativo di non essere più bloccato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prima di mostrare il codice però osserviamo cosa il nostro script dovrà fare. Su Bitinfocharts gli indirizzi hanno una propria pagina, nella quale è presente un riferimento al wallet di cui fanno parte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/BI1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osservando il sorgente della pagina, notiamo che il link al wallet è il primo link dentro la prima tabella di classe \"*table table-striped table-condensed*\" presente nella pagina. Notare che è già presente il nome del wallet, pertanto non è necessario dover andare sulla pagina del wallet per reperirlo (evitando il più possibile captcha)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/BI2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Components.json','r') as infile:\n",
    "    my_components= json.load(infile)\n",
    "my_components_len= [len(c) for c in my_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prendo i 10 cluster più grandi\n",
    "n= 10 #numero cluster\n",
    "\n",
    "chosen_clusters = my_components[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note sullo script:**\n",
    "\n",
    "* BitInfocharts blocca qualunque automaticamente qualunque script/bot osservando lo user-agent. Si rivela quindi necessario cambiarlo, in modo da potersi fingere utenti umani.\n",
    "\n",
    "* Similmente a WalletExplorer, Bitinfocharts ha dei wallet con un nome non significato, e quindi non l'ideale per la deanonimizzazione (in questo caso è un numero intero). Data la difficoltà nel visionare un gran numero di pagine (dovuta ai captcha) lo script permette di impostare un limite di indirizzi da analizzare (parametro *limit*).\n",
    "\n",
    "* Nel caso in cui non si sia trovato nessun nome significativo, viene preso come wallet relativo al cluster quello di cui fanno parte più indirizzi analizzati.\n",
    "\n",
    "* Nel caso di captcha lo script rimarrà bloccato sulla chiamata **requests.get**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_name_bs(cluster,limit=80):\n",
    "    \n",
    "    #user agent da usare per non essere bloccati\n",
    "    headers= {'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:101.0) Gecko/20100101 Firefox/101.0'}\n",
    "    base_url='https://bitinfocharts.com/bitcoin/address/'\n",
    "    \n",
    "    #lista wallet non significativi visti\n",
    "    results_seen=list()\n",
    "    \n",
    "    \n",
    "    for address in cluster:\n",
    "        \n",
    "        if limit == 0 : #se ho raggiunto limit iteraioni\n",
    "            \n",
    "            \n",
    "            if len(results_seen) == 0: #caso estremo, accade se nessun indirizzo passato è presente nel sito\n",
    "                return None\n",
    "            data = Counter(results_seen)\n",
    "            return data.most_common(1)[0][0]\n",
    "        \n",
    "        limit = limit-1\n",
    "        \n",
    "        #richiedo pagina address\n",
    "        r= requests.get(base_url+address,headers=headers)\n",
    "        \n",
    "        try:\n",
    "            r.raise_for_status() #controllo che mi sia stata data effettivamente la pagina\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            continue\n",
    "\n",
    "        bs= BeautifulSoup(r.text,'html.parser')\n",
    "        table= bs.find('table',class_=\"table table-striped table-condensed\") #prendo tabella interessata\n",
    "        a=table.find('a') #prendo link al wallet\n",
    "        \n",
    "        try:\n",
    "            #se il wallet è un numero intero -> non significativo, continua a cercare\n",
    "            int(a.text[a.text.find('wallet: ')+8:])\n",
    "            results_seen.append(a.text[a.text.find('wallet: ')+8:]) #metti wallet in lista\n",
    "            continue\n",
    "        \n",
    "        except ValueError: #non è un numero -> nome significativo\n",
    "            return a.text[a.text.find('wallet: ')+8:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list=list()\n",
    "\n",
    "\n",
    "for idx,cluster in enumerate(chosen_clusters):\n",
    "    print(idx)\n",
    "    names_list.append(solve_name_bs(cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note sullo script:**\n",
    "\n",
    "* il funzionamento è analogo allo script con bs, ma, nell' eventualità di trovare un captcha, lo script con selenium salta al prossimo indirizzo nella speranza di non essere bloccato di nuovo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_name_sel(cluster,limit=80):\n",
    "    base_url='https://bitinfocharts.com/bitcoin/address/'\n",
    "\n",
    "    #commentare  queste 2 righe pervedere finestra browser\n",
    "    options = FirefoxOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    \n",
    "    #uso firefox perchè più veloce sul mio pc\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "\n",
    "    #lista wallet non significativi visti\n",
    "    results_seen=list()\n",
    "\n",
    "    \n",
    "    for address in cluster:\n",
    "        \n",
    "        if limit == 0 :  # iterato limit volte\n",
    "            driver.quit() #chiudi webdriver\n",
    "            \n",
    "            #non sono riuscito a ottenere nessun wallet (causa captcha, pagine inesistenti)\n",
    "            if len(results_seen) == 0: \n",
    "                return None\n",
    "            \n",
    "            #ho visto almeno un wallet\n",
    "            #restituisco wallet più frequente\n",
    "            data = Counter(results_seen)\n",
    "            return data.most_common(1)[0][0]\n",
    "        \n",
    "        \n",
    "        limit= limit-1\n",
    "\n",
    "        driver.get(base_url+address) #richiedi pagina\n",
    "        \n",
    "        #print(driver.title)\n",
    "        \n",
    "        #se ottengo captcha vado avanti\n",
    "        if driver.title ==\"Just a moment...\" or driver.title == \"Ci siamo quasi…\":\n",
    "            continue\n",
    "        \n",
    "        #ottengo link wallet\n",
    "        a= driver.find_element(By.XPATH,'/html/body/div[3]/div[3]/table/tbody/tr/td/table/tbody/tr[1]/td[2]/small/a') \n",
    "        \n",
    "        #prendo il nome nel link\n",
    "        link= a.get_attribute('href') \n",
    "        \n",
    "        try :\n",
    "            \n",
    "            #se il nome è un intero -> non significativo -> mettilo in lista e vai avanti\n",
    "            int(link[link.find('wallet/')+len('wallet/'):])\n",
    "            results_seen.append(link[link.find('wallet/')+len('wallet/'):])\n",
    "            continue\n",
    "            \n",
    "        except ValueError: #è un nome significativo\n",
    "            driver.quit()    \n",
    "            return link[link.find('wallet/')+len('wallet/'):]\n",
    "        \n",
    "        \n",
    "        #driver.quit()    \n",
    "        #return link[link.find('wallet/')+len('wallet/'):]\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per vedere risultati script beautifulsoup non eseguire questa cella. Comunque drovebbero essere uguali\n",
    "\n",
    "names_list=list()\n",
    "\n",
    "\n",
    "for idx,cluster in enumerate(chosen_clusters):\n",
    "    print(idx)\n",
    "    names_list.append(solve_name_sel(cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,(my_len,name) in enumerate(zip(my_components_len,names_list)):\n",
    "    print(f'Cluster {idx} size: {my_len} \\tName: {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ImmaginiLWS/res2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perchè i nomi sono diversi diversi?\n",
    "\n",
    "* WalletExplorer de-anonimizza gli indirizzi utilizzando una procedura di clustering e scraping. Una possibile ragione per cui WalletExplorer potrebbe commettere errori nella de-anonimizzazione è l'euristica utilizzata. Nonostante l'euristica sia scelta per minimizzare gli errori di de-anonimizzazione, potrebbe verificarsi una discrepanza nei nomi rispetto a quelli forniti da Bitinfocharts.\n",
    "\n",
    "* Non è chiaro come Bitinfocharts ottenga i nomi dei wallet. Pertanto, se la fonte o il processo da cui ottengono i nomi è errato, potrebbero esserci differenze nei nomi rispetto a quelli effettivamente associati a quel cluster. Questo significa che i nomi ottenuti tramite WalletExplorer potrebbero essere diversi da quelli ottenuti con Bitinfocharts."
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "report_row_ids": [],
   "version": 3
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
